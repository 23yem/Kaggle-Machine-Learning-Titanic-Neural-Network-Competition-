{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc6158d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:41.999191Z",
     "iopub.status.busy": "2023-11-18T19:46:41.998195Z",
     "iopub.status.idle": "2023-11-18T19:46:42.944019Z",
     "shell.execute_reply": "2023-11-18T19:46:42.942597Z"
    },
    "papermill": {
     "duration": 0.962086,
     "end_time": "2023-11-18T19:46:42.946949",
     "exception": false,
     "start_time": "2023-11-18T19:46:41.984863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d28f85",
   "metadata": {
    "papermill": {
     "duration": 0.010812,
     "end_time": "2023-11-18T19:46:42.969277",
     "exception": false,
     "start_time": "2023-11-18T19:46:42.958465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Load the Data from the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b621049b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:42.994217Z",
     "iopub.status.busy": "2023-11-18T19:46:42.992954Z",
     "iopub.status.idle": "2023-11-18T19:46:43.047702Z",
     "shell.execute_reply": "2023-11-18T19:46:43.046522Z"
    },
    "papermill": {
     "duration": 0.069755,
     "end_time": "2023-11-18T19:46:43.050259",
     "exception": false,
     "start_time": "2023-11-18T19:46:42.980504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remember to eventually clean up the wrong or missing values\n",
    "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc98fe99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:43.076282Z",
     "iopub.status.busy": "2023-11-18T19:46:43.075147Z",
     "iopub.status.idle": "2023-11-18T19:46:43.099740Z",
     "shell.execute_reply": "2023-11-18T19:46:43.098925Z"
    },
    "papermill": {
     "duration": 0.040054,
     "end_time": "2023-11-18T19:46:43.102044",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.061990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remember to eventually clean up the wrong or missing values\n",
    "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4ce030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:43.129519Z",
     "iopub.status.busy": "2023-11-18T19:46:43.128749Z",
     "iopub.status.idle": "2023-11-18T19:46:43.134823Z",
     "shell.execute_reply": "2023-11-18T19:46:43.133725Z"
    },
    "papermill": {
     "duration": 0.022998,
     "end_time": "2023-11-18T19:46:43.137528",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.114530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadac895",
   "metadata": {
    "papermill": {
     "duration": 0.012206,
     "end_time": "2023-11-18T19:46:43.162615",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.150409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Choosing features #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99dfc56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:43.188995Z",
     "iopub.status.busy": "2023-11-18T19:46:43.188323Z",
     "iopub.status.idle": "2023-11-18T19:46:43.223005Z",
     "shell.execute_reply": "2023-11-18T19:46:43.221524Z"
    },
    "papermill": {
     "duration": 0.051339,
     "end_time": "2023-11-18T19:46:43.226085",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.174746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Fare  Sex_female  Sex_male\n",
       "0       3   7.2500       False      True\n",
       "1       1  71.2833        True     False\n",
       "2       3   7.9250        True     False\n",
       "3       1  53.1000        True     False\n",
       "4       3   8.0500       False      True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"Pclass\", \"Sex\", \"Fare\"]\n",
    "\n",
    "X_train = pd.get_dummies(train_data[features])\n",
    "\n",
    "X_train.head()\n",
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85993bac",
   "metadata": {
    "papermill": {
     "duration": 0.011881,
     "end_time": "2023-11-18T19:46:43.250469",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.238588",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d796fc3b",
   "metadata": {
    "papermill": {
     "duration": 0.01178,
     "end_time": "2023-11-18T19:46:43.274572",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.262792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set the target/label (y) values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c754b5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:43.301630Z",
     "iopub.status.busy": "2023-11-18T19:46:43.300452Z",
     "iopub.status.idle": "2023-11-18T19:46:43.309009Z",
     "shell.execute_reply": "2023-11-18T19:46:43.307885Z"
    },
    "papermill": {
     "duration": 0.024764,
     "end_time": "2023-11-18T19:46:43.311783",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.287019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y_train = pd.get_dummies(train_data[\"Survived\"])\n",
    "Y_train = train_data[\"Survived\"]\n",
    "\n",
    "Y_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba83b1",
   "metadata": {
    "papermill": {
     "duration": 0.011841,
     "end_time": "2023-11-18T19:46:43.336251",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.324410",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9cb28fb",
   "metadata": {
    "papermill": {
     "duration": 0.01194,
     "end_time": "2023-11-18T19:46:43.360649",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.348709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Split up train data into Train-test using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1eb1a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:43.387329Z",
     "iopub.status.busy": "2023-11-18T19:46:43.386543Z",
     "iopub.status.idle": "2023-11-18T19:46:44.923474Z",
     "shell.execute_reply": "2023-11-18T19:46:44.921693Z"
    },
    "papermill": {
     "duration": 1.553295,
     "end_time": "2023-11-18T19:46:44.926304",
     "exception": false,
     "start_time": "2023-11-18T19:46:43.373009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445    1\n",
      "650    0\n",
      "172    1\n",
      "450    0\n",
      "314    0\n",
      "Name: Survived, dtype: int64\n",
      "709    1\n",
      "439    0\n",
      "840    0\n",
      "720    1\n",
      "39     1\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split data into a train/validation and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 42)\n",
    "\n",
    "#print(X_train.head())\n",
    "#print(Y_train.head())\n",
    "print(Y_train.head())\n",
    "\n",
    "print(Y_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a862f",
   "metadata": {
    "papermill": {
     "duration": 0.012278,
     "end_time": "2023-11-18T19:46:44.951248",
     "exception": false,
     "start_time": "2023-11-18T19:46:44.938970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "spliting the data into a train-test split will allow us to test our model and so it will allow us to tune our model before submitting on Kaggle. We can also use the train-validation-test split here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461aa9f4",
   "metadata": {
    "papermill": {
     "duration": 0.012043,
     "end_time": "2023-11-18T19:46:44.976000",
     "exception": false,
     "start_time": "2023-11-18T19:46:44.963957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Making the Neural Network Model using Scikit Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e97267c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:45.003060Z",
     "iopub.status.busy": "2023-11-18T19:46:45.002621Z",
     "iopub.status.idle": "2023-11-18T19:46:45.508096Z",
     "shell.execute_reply": "2023-11-18T19:46:45.506386Z"
    },
    "papermill": {
     "duration": 0.522165,
     "end_time": "2023-11-18T19:46:45.510755",
     "exception": false,
     "start_time": "2023-11-18T19:46:44.988590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(5, 5), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(Y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd70f3",
   "metadata": {
    "papermill": {
     "duration": 0.012136,
     "end_time": "2023-11-18T19:46:45.535637",
     "exception": false,
     "start_time": "2023-11-18T19:46:45.523501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Here I am using the MLPClassifier since this is a classification problem. The output of the model will be \"Survive\" or \"not survive.\" If my model was supposed to predict a numerical value, I would be using something like MLPRegressor instead**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd6673",
   "metadata": {
    "papermill": {
     "duration": 0.012115,
     "end_time": "2023-11-18T19:46:45.560637",
     "exception": false,
     "start_time": "2023-11-18T19:46:45.548522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tried making a neural network using Tensor Flow but decided to just use Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac08a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:45.588167Z",
     "iopub.status.busy": "2023-11-18T19:46:45.587486Z",
     "iopub.status.idle": "2023-11-18T19:46:45.592909Z",
     "shell.execute_reply": "2023-11-18T19:46:45.591727Z"
    },
    "papermill": {
     "duration": 0.022447,
     "end_time": "2023-11-18T19:46:45.595584",
     "exception": false,
     "start_time": "2023-11-18T19:46:45.573137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "# #Compile the model\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Dense(64, activation = \"relu\", input_shape = (687,)),\n",
    "#     tf.keras.layers.Dense(32, activation = \"relu\"),\n",
    "#     tf.keras.layers.Dense(16, activation = \"relu\"),\n",
    "#     tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "# ])\n",
    "# model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "# #Train the model\n",
    "\n",
    "# #X_train = tf.convert_to_tensor(X_train)\n",
    "# #Y_train = tf.convert_to_tensor(Y_train)\n",
    "# model.fit(X_train, Y_train, epochs = 10)\n",
    "\n",
    "# #Evaluate the model \n",
    "\n",
    "# #X_test = tf.convert_to_tensor(X_test)\n",
    "# #Y_test = tf.convert_to_tensor(Y_test)\n",
    "# loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "# print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801eb76",
   "metadata": {
    "papermill": {
     "duration": 0.01208,
     "end_time": "2023-11-18T19:46:45.620294",
     "exception": false,
     "start_time": "2023-11-18T19:46:45.608214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing model on real test set and Submitting Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd4b393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:45.647588Z",
     "iopub.status.busy": "2023-11-18T19:46:45.647151Z",
     "iopub.status.idle": "2023-11-18T19:46:45.687096Z",
     "shell.execute_reply": "2023-11-18T19:46:45.685704Z"
    },
    "papermill": {
     "duration": 0.056514,
     "end_time": "2023-11-18T19:46:45.689622",
     "exception": false,
     "start_time": "2023-11-18T19:46:45.633108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Fare', 'Sex_female', 'Sex_male', 'PassengerId'], dtype='object')\n",
      "(418, 4)\n",
      "(418,)\n",
      "(418, 5)\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "features = [\"Pclass\", \"Sex\", \"Fare\"]\n",
    "test_X = pd.get_dummies(test_data[features])\n",
    "#test_X = test_X.dropna()\n",
    "#print(test_X[\"Sex_female\"])\n",
    "test_X['Fare'].fillna(test_X['Fare'].median(), inplace=True)\n",
    "\n",
    "# Impute categorical columns with the mode\n",
    "for column in ['Pclass']:\n",
    "    test_X[column].fillna(test_X[column].mode()[0], inplace=True)\n",
    "\n",
    "for column in [\"Sex_female\"]:\n",
    "    test_X[column].fillna(False, inplace=True)\n",
    "    \n",
    "for column in ['Sex_male']:\n",
    "    test_X[column].fillna(True, inplace=True)\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "# print(test_data)\n",
    "# print(X_test)\n",
    "\n",
    "#model.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "#test_data = test_data.dropna(subset=['Pclass', 'Sex', 'Fare'])\n",
    "\n",
    "pass_id = test_data['PassengerId']\n",
    "\n",
    "test_data = test_data.drop('PassengerId', axis=1)\n",
    "test_data = pd.get_dummies(test_data[features])\n",
    "\n",
    "test_data[\"PassengerId\"] = pass_id\n",
    "\n",
    "test_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)\n",
    "\n",
    "print(test_data.columns)\n",
    "\n",
    "# Impute categorical columns with the mode\n",
    "for column in ['Pclass']:\n",
    "    test_data[column].fillna(test_data[column].mode()[0], inplace=True)\n",
    "\n",
    "for column in ['Sex_female']:\n",
    "    test_data[column].fillna(False, inplace=True)\n",
    "    \n",
    "for column in ['Sex_male']:\n",
    "    test_data[column].fillna(True, inplace=True)\n",
    "\n",
    "print(test_X.shape)\n",
    "print(predictions.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4d93d",
   "metadata": {
    "papermill": {
     "duration": 0.012355,
     "end_time": "2023-11-18T19:46:45.715187",
     "exception": false,
     "start_time": "2023-11-18T19:46:45.702832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# After first submission, I will now try and edit the neural network to improve the accurary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba69cece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:45.742889Z",
     "iopub.status.busy": "2023-11-18T19:46:45.742471Z",
     "iopub.status.idle": "2023-11-18T19:46:46.078995Z",
     "shell.execute_reply": "2023-11-18T19:46:46.074686Z"
    },
    "papermill": {
     "duration": 0.354887,
     "end_time": "2023-11-18T19:46:46.083105",
     "exception": false,
     "start_time": "2023-11-18T19:46:45.728218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(14, 4), activation='relu', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(Y_test, y_pred):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f5e80",
   "metadata": {
    "papermill": {
     "duration": 0.012741,
     "end_time": "2023-11-18T19:46:46.185737",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.172996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**I edited  the hidden layer sizes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de7646",
   "metadata": {
    "papermill": {
     "duration": 0.012473,
     "end_time": "2023-11-18T19:46:46.210995",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.198522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Second submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131ce33f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:46.238762Z",
     "iopub.status.busy": "2023-11-18T19:46:46.238298Z",
     "iopub.status.idle": "2023-11-18T19:46:46.271011Z",
     "shell.execute_reply": "2023-11-18T19:46:46.269539Z"
    },
    "papermill": {
     "duration": 0.049843,
     "end_time": "2023-11-18T19:46:46.273600",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.223757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Index(['Pclass', 'Fare', 'Sex_female', 'Sex_male', 'PassengerId'], dtype='object')\n",
      "(418, 4)\n",
      "(418,)\n",
      "(418, 5)\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "features = [\"Pclass\", \"Sex\", \"Fare\"]\n",
    "test_X = pd.get_dummies(test_data[features])\n",
    "\n",
    "test_X['Fare'].fillna(test_X['Fare'].median(), inplace=True)\n",
    "\n",
    "# Impute categorical columns with the mode\n",
    "for column in ['Pclass']:\n",
    "    test_X[column].fillna(test_X[column].mode()[0], inplace=True)\n",
    "\n",
    "for column in [\"Sex_female\"]:\n",
    "    test_X[column].fillna(False, inplace=True)\n",
    "    \n",
    "for column in ['Sex_male']:\n",
    "    test_X[column].fillna(True, inplace=True)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "predictions2 = model.predict(test_X)\n",
    "\n",
    "pass_id = test_data['PassengerId']\n",
    "\n",
    "test_data = test_data.drop('PassengerId', axis=1)\n",
    "test_data = pd.get_dummies(test_data[features])\n",
    "\n",
    "test_data[\"PassengerId\"] = pass_id\n",
    "\n",
    "print(test_data.columns)\n",
    "\n",
    "print(test_X.shape)\n",
    "print(predictions2.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions2})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ccf448",
   "metadata": {
    "papermill": {
     "duration": 0.01801,
     "end_time": "2023-11-18T19:46:46.307934",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.289924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Here, from the code above, I went from a 0.755 to a 0.76 by editing the hidden layer sizes. However, this wasn't much of a big change and the accuracy of the model is still worse than the 0.775 which I achieved through a simple Random Decision Forest Model.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b1b3a",
   "metadata": {
    "papermill": {
     "duration": 0.016457,
     "end_time": "2023-11-18T19:46:46.340569",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.324112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**From this code, I realized that I should be imputing and cleaning the data BEFORE I use pd.get_dummies(). This is because pd.get_dummies() adds a lot more columns to the data because it turns all categorial columns to multiple binary columns which puts a 0 or 1 in each row. So, I should be cleaming up the data BEFORE I use pd.get_dummies() since pd.get_dummies adds so many more columns to the data and makes the data so much harder to clean. \n",
    "For example, it's hard to impute values in the \"age\" column after you do pd.get_Dummies() since imputing age relies on finding the average age of each person and it's a lot easier to find the average age is the all the ages were in a single column but if we do pd.get_Dummies, we are going to split up the age column into many more age columns which correspond to each unique age value in the data set**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6759a4",
   "metadata": {
    "papermill": {
     "duration": 0.012493,
     "end_time": "2023-11-18T19:46:46.366426",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.353933",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e07fa651",
   "metadata": {
    "papermill": {
     "duration": 0.014906,
     "end_time": "2023-11-18T19:46:46.394317",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.379411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Now I will re-initialize my data set and clean up my data before using pd.get_dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801ee65",
   "metadata": {
    "papermill": {
     "duration": 0.012892,
     "end_time": "2023-11-18T19:46:46.421043",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.408151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**I set up X_train and Y_train, but this time, instead of doing pd.get_Dummies() on X_train, I'm not going to use pd.get_Dummies() until after I clean up the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb532f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:46.449080Z",
     "iopub.status.busy": "2023-11-18T19:46:46.448360Z",
     "iopub.status.idle": "2023-11-18T19:46:46.468668Z",
     "shell.execute_reply": "2023-11-18T19:46:46.467097Z"
    },
    "papermill": {
     "duration": 0.037313,
     "end_time": "2023-11-18T19:46:46.471255",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.433942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "\n",
    "features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"] # Here, I also added more features which I think might be useful\n",
    "\n",
    "X_train = train_data # I won't actually limit the X_train to just the columns of the features I want just yet. This is because I will use the other columns in the data set to help impute missing values\n",
    "\n",
    "Y_train = train_data[\"Survived\"]\n",
    "\n",
    "print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c20ce",
   "metadata": {
    "papermill": {
     "duration": 0.012579,
     "end_time": "2023-11-18T19:46:46.497171",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.484592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "778c44f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:46.525132Z",
     "iopub.status.busy": "2023-11-18T19:46:46.524642Z",
     "iopub.status.idle": "2023-11-18T19:46:46.538241Z",
     "shell.execute_reply": "2023-11-18T19:46:46.537126Z"
    },
    "papermill": {
     "duration": 0.030461,
     "end_time": "2023-11-18T19:46:46.540667",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.510206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values for Pclass\n",
    "\n",
    "average_Pclass = X_train[\"Pclass\"].median()\n",
    "\n",
    "X_train[\"Pclass\"].fillna(average_Pclass, inplace = True)\n",
    "\n",
    "print(X_train[\"Pclass\"].isnull().sum()) # print the number of rows in the \"Pclass\" columns that now have no value\n",
    "\n",
    "print(X_train[\"Sex\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Sex\" column so we don't need to impute anything\n",
    "\n",
    "print(X_train[\"Fare\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Fare\" column so we don't need to impute anything\n",
    "\n",
    "print(X_train[\"SibSp\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"SibSp\" column so we don't need to impute anything\n",
    "\n",
    "print(X_train[\"Parch\"].isnull().sum()) # Since this prints out 0, this means that there is no missing value in the \"Parch\" column so we don't need to impute anything\n",
    "\n",
    "print(X_train[\"Age\"].isnull().sum()) # This prints out 177 so there are 177 rows with missing values in the \"Age\" column! This means we have to do a lot of imputing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9af31a",
   "metadata": {
    "papermill": {
     "duration": 0.01308,
     "end_time": "2023-11-18T19:46:46.567046",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.553966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**From this, we learned that age is the main column with missing values (of the columns that we are working with) so we need to methodically impute those missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d126ce0",
   "metadata": {
    "papermill": {
     "duration": 0.013236,
     "end_time": "2023-11-18T19:46:46.593392",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.580156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cleaning up (imputing) the Age column using a Random Forest Regressor model (a type of Machine Learning model) that accurately predicts the age of a person given their "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2557f2ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:46.625528Z",
     "iopub.status.busy": "2023-11-18T19:46:46.623844Z",
     "iopub.status.idle": "2023-11-18T19:46:49.509287Z",
     "shell.execute_reply": "2023-11-18T19:46:49.507902Z"
    },
    "papermill": {
     "duration": 2.905537,
     "end_time": "2023-11-18T19:46:49.512463",
     "exception": false,
     "start_time": "2023-11-18T19:46:46.606926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#We use \"LabelEncoder\" to turn all the categorial values (values which don't use numbers) into values which the Random Forest Regressor can understand more easily. This is very similar to doing something like pd.get_dummies()\n",
    "\n",
    "label_encoders = {}\n",
    "for column in ['Sex', 'Embarked']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    # We use 'astype(str)' to convert any NaN values to a string representation\n",
    "    X_train[column] = label_encoders[column].fit_transform(X_train[column].astype(str))\n",
    "\n",
    "features = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "# One very important thing that I will end up doing is using the \"Survived\" column of the dataset to predict the age. This is because there is a strong correleation between the survival and age. \n",
    "\n",
    "\n",
    "#This is an imputer. It is a function that can efficiently impute values in the way you want. For this scenario, I am using IterativeImputer() and the RandomForestRegressor(() to )\n",
    "imputer = IterativeImputer(RandomForestRegressor(n_estimators=10), max_iter=10, random_state=0)\n",
    "\n",
    "\n",
    "# Fit the imputer on the DataFrame with the features\n",
    "imputer.fit(X_train[features]) #When IterativeImputer is used alongside an ML model like RandomForestRegressor, calling .fit() on the IterativeImputer() object will also fit (train) the ML model\n",
    "\n",
    "\n",
    "# Perform the imputation on the Training Data. THIS WILL ACTUAL IMPUTE ANY MISSING VALUES IN EACH ROW, not only the missing values in Age.\n",
    "X_train_imputed = imputer.transform(X_train[features])\n",
    "\n",
    "\n",
    "# Convert the output back to a DataFrame\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=features)\n",
    "\n",
    "# Update the original DataFrame with the imputed values. This means that the columns of the original X_train dataframe that weren't part of the features (like \"Name\" isn't apart of the features list)\n",
    "X_train[features] = X_train_imputed\n",
    "\n",
    "#print(X_train.head())\n",
    "\n",
    "print(X_train[\"Age\"].isnull().sum()) # Since this prints 0, we now know that there are 0 missing values in the age column and so the imputation was successful "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fded81b",
   "metadata": {
    "papermill": {
     "duration": 0.013917,
     "end_time": "2023-11-18T19:46:49.540273",
     "exception": false,
     "start_time": "2023-11-18T19:46:49.526356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Using IterativeRegressor() and RandomForestRegressor(), I essentially used an ML model to impute any missing values in the dataset. I wanted to clean up the age column using the most effective method and I didn't want to just find the average age and impute all those values to the missing values. I wanted to be smart since age is an incredibly important feature.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b40c17",
   "metadata": {
    "papermill": {
     "duration": 0.013156,
     "end_time": "2023-11-18T19:46:49.566960",
     "exception": false,
     "start_time": "2023-11-18T19:46:49.553804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Using the ImperativeImputer(), this will actually impute all the missing values in each row. It won't only impute the missing values in the \"age\" column. BUT, since we already cleaned the data and know there are 0 missing values for all the other columns that we care about (the ones with the features we are using), the imputer will be imputing values only to the missing values of age and other columns that we don't care about, but out of all the columns we care about, it will only be affecting the age column since all the other columns we care about have 0 missing values as we checked in the code previously.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83beb1e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:46:49.596092Z",
     "iopub.status.busy": "2023-11-18T19:46:49.595367Z",
     "iopub.status.idle": "2023-11-18T19:46:49.610731Z",
     "shell.execute_reply": "2023-11-18T19:46:49.609509Z"
    },
    "papermill": {
     "duration": 0.033082,
     "end_time": "2023-11-18T19:46:49.613467",
     "exception": false,
     "start_time": "2023-11-18T19:46:49.580385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass  Sex     Fare   Age  SibSp  Parch\n",
      "0       3.0  1.0   7.2500  22.0    1.0    0.0\n",
      "1       1.0  0.0  71.2833  38.0    1.0    0.0\n",
      "2       3.0  0.0   7.9250  26.0    0.0    0.0\n",
      "3       1.0  0.0  53.1000  35.0    1.0    0.0\n",
      "4       3.0  1.0   8.0500  35.0    0.0    0.0\n",
      "..      ...  ...      ...   ...    ...    ...\n",
      "886     2.0  1.0  13.0000  27.0    0.0    0.0\n",
      "887     1.0  0.0  30.0000  19.0    0.0    0.0\n",
      "888     3.0  0.0  23.4500  29.7    1.0    2.0\n",
      "889     1.0  1.0  30.0000  26.0    0.0    0.0\n",
      "890     3.0  1.0   7.7500  32.0    0.0    0.0\n",
      "\n",
      "[891 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "features = [\"Pclass\", \"Sex\", \"Fare\",\"Age\", \"SibSp\", \"Parch\"]\n",
    "\n",
    "X_train = X_train[features]\n",
    "\n",
    "print(X_train)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30579,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.147838,
   "end_time": "2023-11-18T19:46:50.249032",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-18T19:46:38.101194",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
